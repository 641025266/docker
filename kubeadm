kubeadm安装问题总结：
1)k8s工具包的来源，也就是kubelet，kubeadm，kubernetes-cni，kubectl这4个从哪里来？
  来源有三种方式：博主提供、官方源安装和release工程编译，非官方源中提供的版本比较老，如果要使用新版本，可以尝试release工程编译的方式或者用博主提供的包下载。
  我选择release工程编译，release地址是https://github.com/kubernetes/release
  本次编译完成的1.9.0的rpm包已经保存在本地
2)k8s安装中依赖到的镜像问题，这些镜像都是google服务器上的镜像，解决方式有如下三个
  2.1)自己有一台国外服务器，通过国外服务器docker pull然后save，然后scp到本地
  2.2)把k8s所在主机的docker配置成为代理形式，通过代理服务器去下载google镜像，但是本质上还是需要一台国外服务器
  2.3)通过https://hub.docker.com/做中转站去下载google服务器上的镜像，只需要创建一个很简单的Dockerfile即可，然后再从该中转站中下载镜像
      详细操作参照https://www.kubernetes.org.cn/878.html
3)只要k8s的版本一样，不管是手动安装还是kubeadm安装，比如都是1.9或者1.10，那么通过不同的yaml配置文件就可以实现不同的效果,dashboard的配置就是一个典型的例子
4)k8s高可用的案例可参照如下两篇文章
  https://www.kubernetes.org.cn/3808.html
  http://dab5690a.wiz03.com/share/s/3qJmAa1FO17x2Rpg0z1ZXVvc1Zdzne1mbAFX2_4WHQ0Hh90H 密码是x0a2
5)k8s的网络组件选择很多，可以根据自己的需要选择calico、weave、flannel，calico性能最好，flannel的vxlan也不错，默认的UDP性能较差，weave的性能比较差，测试环境用下可以，
  本次安装选择flannel
6)k8s升级，暂时没有遇到


本次简单安装，一master一个node,主要参照http://cloudnil.com/2017/07/10/Deploy-kubernetes1.6.7-with-kubeadm/
172.16.68.138 node1   扮演master节点
172.16.68.134 node2   扮演node节点
安装流程
1) 环境初始化，所有master和node节点都需进行
]# yum install -y epel-release
]# swapoff -a  同时打开/etc/fstab，把swap这一行删除掉
]# systemctl stop firewalld
]# systemctl disable firewalld
还要确保selinux处于disable状态
]# vi /etc/sysct.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
vm.swappiness = 0
vm.overcommit_memory=1
vm.panic_on_oom=0
kernel/panic=10
kernel/panic_on_oops=1
kernel.pid_max = 4194303
vm.max_map_count = 655350
fs.aio-max-nr = 524288
fs.file-max = 6590202
net.ipv4.neigh.default.gc_stale_time=120
net.ipv4.conf.lo.arp_announce=2
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.arp_announce=2
net.ipv4.conf.all.arp_announce=2
net.ipv4.ip_forward = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
]# sysctl -p /etc/sysctl.conf
环境初始化完后，最好重启下服务器
使用kubeadm来安装k8s集群，除了etcd是单节点外，其余组件自动实现高可用，所以etcd需要单独安装
2)k8s工具包安装,所有master和node节点都需要进行
]# yum install -y kubeadm-1.9.0-0.x86_64.rpm kubectl-1.9.0-0.x86_64.rpm kubelet-1.9.0-0.x86_64.rpm kubernetes-cni-0.6.0-0.x86_64.rpm
]# systemctl enable kubelet.service
]# systemctl start  kubelet.service
]# yum install -y docker  如果有必要，可以配置docker代理来访问google服务器，线上环境一般是配置docker访问事先做好的私有仓库
]# systemctl enable docker.service
]# systemctl start  docker.service
3)在所有master节点初始化，本次就是172.16.68.138节点,如果有多个master节点做高可用，那所有的master节点都要执行
]# docker images   初始化之前要确保kubeadm依赖到的如下4个基本镜像是存在，这样可以缩短初始化时间。镜像来源可以参照上面的总结进行
gcr.io/google_containers/kube-apiserver-amd64                        v1.9.0              
gcr.io/google_containers/kube-controller-manager-amd64               v1.9.0             
gcr.io/google_containers/kube-scheduler-amd64                        v1.9.0
gcr.io/google_containers/pause-amd64                                 3.0
]# vi config.yaml  提供初始的配置文件
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
etcd:
  endpoints:
  - http://172.16.68.138:2379
  dataDir: /root/etcd/data/etcd-1.etcd
networking:
  podSubnet: 10.244.0.0/16  这里的网段要和后续的flannel.yaml里面的保持一致
kubernetesVersion: 1.9.0
api:
  advertiseAddress: "172.16.68.138"
token: "b99a00.a144ef80536d4344"
tokenTTL: "0s"
apiServerCertSANs:
- node1
- 172.16.68.138
featureGates:
  CoreDNS: true
]# kubeadm reset 这个操作适合初始化操作失败后重置或者在初始化之前进行
]# kubeadm init --config config.yaml  此处会花费几分钟,初始化成功后会出现如下提示信息，在初始化之前务必执行systemctl start kubelet.service，即使失败也要执行
   .......
   Your Kubernetes master has initialized successfully!
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of machines by running the following on each node as root:
   kubeadm join --token b99a00.a144ef80536d4344 172.16.68.138:6443 --discovery-token-ca-cert-hash sha256:5d078523df0d90bf7c2e9875f50a0809d8a435638d189aa09ac6264a13011900
   务必要把以上这条命令保存起来，后续增加其他节点会使用到
]# mkdir -p $HOME/.kube
]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
]# chown $(id -u):$(id -g) $HOME/.kube/config
]# kubectl get nodes   此时只有master，而且处于NotReady状态，是因为还没有部署网络的原因
NAME      STATUS       ROLES     AGE       VERSION
node1     NotReady     master    19h       v1.9.0
]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
]# vi kube-flannel.yml   
   ---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-amd64
        imagePullPolicy: IfNotPresent
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg

]# kubectl create -f kube-flannel.yml 依赖到quay.io/coreos/flannel:v0.10.0-amd64需要提前准备好
]# kubectl get nodes  再次查看节点状态，应该处于Ready状态
NAME      STATUS    ROLES     AGE       VERSION
node1     Ready     master    19h       v1.9.0
]# kubectl get pod --all-namespaces 查看所有组件运行状态，差镜像的要提供镜像，确保所有组件是running
]# kubectl get services --all-namespaces
]# kubectl get secret --all-namespaces
4)把所有node节点加入到k8s集群中，本次就是172.16.68.134节点
]# docker images  在加入集群前，要先确保flannel镜像存在 
REPOSITORY                                                           TAG                 IMAGE ID            CREATED             SIZE
quay.io/coreos/flannel                                               v0.10.0-amd64
]# kubeadm reset 
]# systemctl start kubelet.service 即使失败也要执行
]# kubeadm join --token b99a00.a144ef80536d4344 172.16.68.138:6443 --discovery-token-ca-cert-hash sha256:5d078523df0d90bf7c2e9875f50a0809d8a435638d189aa09ac6264a13011900
然后到master节点执行kubectl get nodes确保所有节点处于Ready状态，至此kubeadm安装k8s集群基本完成，下面依次安装dashboard，ingress


5)dashboard安装
同样的镜像版本，通过不同的yaml配置文件可以实现不同的效果，本次的dashboard实现的是http://ip:port来访问，这样可以通过后续的ingress配置域名访问,
如果实现的是https://ip:port,则不能通过后续的ingress配置域名访问，原因未知
]# vi kubernetes-dashboar.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: default
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        image: registry.cn-hangzhou.aliyuncs.com/k8sth/kubernetes-dashboard-amd64:v1.8.3
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9090
          protocol: TCP
        args:
        livenessProbe:
          httpGet:
            path: /
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: default
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 9090
  selector:
    k8s-app: kubernetes-dashboard


ingress，https的安装可以参照手动安装的资料进行


